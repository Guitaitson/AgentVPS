# Avalia√ß√£o T√©cnica v3 ‚Äî AgentVPS ‚Äî 15 Fevereiro 2026

## Contexto

Terceira avalia√ß√£o ap√≥s Sprint 260214 (Skill Registry + 10 Skills + Autonomous Engine). O projeto foi reavaliado lendo todos os 77 arquivos Python (~16.930 linhas) no reposit√≥rio GitHub.

---

## 1. O Que Mudou Desde a v2

### Entregas Concretas

| Item Planejado | Status | Observa√ß√£o |
|---|---|---|
| S1: Skill Registry (base.py + registry.py) | ‚úÖ Entregue | 203 linhas, auto-discovery funcional |
| S1: Migrar 5 tools builtin para skills | ‚úÖ Entregue | ram, containers, system_status, check_postgres, check_redis |
| S2-01: shell_exec | ‚úÖ Entregue | 397 linhas com classifica√ß√£o SAFE/MODERATE/DANGEROUS/FORBIDDEN |
| S2-02: file_manager | ‚úÖ Entregue | 188 linhas com path validation |
| S2-03: web_search | ‚úÖ Entregue | 97 linhas com Brave Search API |
| S2-04: memory_query | ‚úÖ Entregue | 124 linhas |
| S2-05: self_edit | ‚úÖ Entregue | 95 linhas |
| S3-01: Deletar intent_classifier.py (571 linhas) | ‚ö†Ô∏è Parcial | Conte√∫do substitu√≠do por coment√°rio DEPRECATED (11 linhas), mas arquivo N√ÉO deletado |
| S3-01: Deletar semantic_memory.py (256 linhas) | ‚ö†Ô∏è Parcial | Conte√∫do substitu√≠do por coment√°rio DEPRECATED (9 linhas), mas arquivo N√ÉO deletado |
| S3-01: Remover system_tools.py TOOLS_REGISTRY | ‚ùå N√£o feito | Ainda 434 linhas, TOOLS_REGISTRY intacto |
| S3-02: Convergir Bot ‚Üí Gateway | ‚ùå N√£o feito | Dois entry points coexistem |
| S4-01: Tabelas PostgreSQL (proposals, missions, policies) | ‚ùå N√£o feito | init-db.sql inalterado |
| S4-02: Autonomous Loop engine | ‚ö†Ô∏è Parcial | engine.py existe (372 linhas) mas diverge do plano. Sem tabelas PostgreSQL. |
| S4-03: 3 Triggers iniciais | ‚ö†Ô∏è Parcial | 6 triggers registrados mas n√£o usam proposals/missions/cap gates |
| S1-03: test_skill_registry.py | ‚úÖ Entregue | 175 linhas |
| node_execute refatorado | ‚úÖ Entregue | Delega√ß√£o ao registry funcional |

### M√©tricas Comparativas

| M√©trica | Avalia√ß√£o v2 | Avalia√ß√£o v3 | Mudan√ßa |
|---|---|---|---|
| Total linhas Python | 15.347 | 16.930 | +1.583 (+10%) |
| Linhas core/ | 11.871 | 13.279 | +1.408 (+12%) |
| Linhas testes/ | 2.419 | 2.594 | +175 (+7%) |
| Arquivos Python core/ | ~40 | 77 | +37 (skills dirs) |
| Skills funcionais | 5 (hardcoded) | 10 (via registry) | +5 novas capacidades |
| Commits | 21 | 22 | +1 (squashed?) |

---

## 2. Problemas Cr√≠ticos Encontrados

### üî¥ CR√çTICO: API Key Exposta no C√≥digo

```python
# core/skills/_builtin/web_search/handler.py, linha 10
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY", "BSA1oVa6QVwZf5E3lCRo1h19cmY9Ywo")
```

Uma API key real est√° hardcoded como valor default e commitada no GitHub p√∫blico. Isso deve ser corrigido imediatamente ‚Äî remover o default, revogar a key atual, gerar uma nova, e usar apenas vari√°veis de ambiente.

### üî¥ CR√çTICO: shell_exec √© um Mega-M√≥dulo Anti-Pattern (397 linhas)

O `shell_exec/handler.py` cresceu para 397 linhas ‚Äî mais que o dobro do registry inteiro (203 linhas). Cont√©m:

1. Classifica√ß√£o de seguran√ßa por regex (~50 linhas) ‚Äî OK
2. Execu√ß√£o de subprocesso (~40 linhas) ‚Äî OK
3. Formata√ß√£o de resposta conversacional com 15 blocos if/elif hardcoded (~120 linhas) ‚Äî PROBLEMA
4. Extra√ß√£o de nome de programa com patterns hardcoded (~30 linhas) ‚Äî PROBLEMA
5. Interpreta√ß√£o LLM para converter linguagem natural em comando (~100 linhas) com heur√≠sticas hardcoded (~50 linhas) ‚Äî PROBLEMA FUNDAMENTAL

O skill faz 3 trabalhos distintos: interpretar a inten√ß√£o, executar o comando, e formatar a resposta. Isso deveria ser responsabilidade do grafo LangGraph (interpretar) e do LLM (formatar), n√£o de um √∫nico handler.

Pior: a fun√ß√£o `_interpret_and_generate_command` reinventa o trabalho que o `node_classify_intent` e o `node_plan` j√° fazem. O grafo classifica a inten√ß√£o com LLM ‚Üí planeja qual skill usar ‚Üí executa o skill. Mas dentro do skill, todo esse trabalho √© refeito com heur√≠sticas hardcoded + outra chamada LLM. S√£o duas chamadas LLM por mensagem, duplicando custo e lat√™ncia.

### Este √© exatamente o problema que voc√™ identificou na sua mensagem ‚Äî "estou criando bot√µes pr√©-codificados."

Cada bloco `if "tem o" in user_input_lower` dentro do shell_exec √© um bot√£o hardcoded. Quando algu√©m pedir "tem o Node.js?" funciona. Quando pedir "Node.js est√° na m√°quina?" n√£o funciona. O agente deveria entender qualquer formula√ß√£o da mesma pergunta via LLM, n√£o via string matching.

### üü° IMPORTANTE: Autonomous Engine Divergiu do Blueprint

O plano definia: tabelas PostgreSQL ‚Üí proposals ‚Üí cap gates ‚Üí miss√µes ‚Üí eventos. O que foi implementado: triggers com `condition: lambda ‚Üí True` que rodam em loop sem proposal/cap gate/evento. Os triggers s√£o cron jobs simples, n√£o o loop aut√¥nomo de 6 passos.

Exemplo do health_check trigger:
```python
Trigger(
    name="health_check",
    condition=lambda: True,   # Sempre roda
    action=health_check_action,
    interval=60,              # A cada 60 segundos
)
```

Isso √© um cron, n√£o um agente aut√¥nomo. N√£o cria proposals. N√£o passa por cap gates. N√£o emite eventos. N√£o re-trigera. Falta a arquitetura inteira que diferencia "cron job" de "autonomous loop."

O trigger `ram_high` √© o mais pr√≥ximo do blueprint: verifica condi√ß√£o (RAM > 80%), salva proposal no Redis. Mas n√£o existe nenhum sistema que processa essa proposal, verifica cap gates, ou a transforma em miss√£o.

### üü° IMPORTANTE: node_security_check Escreve Debug Log em Arquivo

```python
# nodes.py, dentro de node_security_check
with open("/tmp/security_debug.log", "a") as f:
    f.write(json.dumps(debug_info) + "\n")
```

Isso est√° no fluxo principal de cada mensagem. Em produ√ß√£o, esse arquivo cresce indefinidamente. Al√©m disso, informa√ß√µes potencialmente sens√≠veis (comandos do usu√°rio) s√£o escritas em plaintext em /tmp.

### üü° IMPORTANTE: system_tools.py N√£o Foi Limpo

434 linhas de c√≥digo com `TOOLS_REGISTRY` intacto. O plano dizia "DEPRECAR ap√≥s S1". O registry de skills funciona, mas `node_execute` ainda importa `system_tools` como fallback:

```python
# nodes.py, linha 399
from ..tools.system_tools import get_async_tool as legacy_get_async_tool
```

Dois registries coexistem.

### üü° IMPORTANTE: Caractere Unicode Corrompido no self_edit

```python
# core/skills/_builtin/self_edit/handler.py, linha 52
if "/opt/vps-agent/" in abs_path:  # cont√©m caractere Ë∑ØÂæÑ
```

O coment√°rio na valida√ß√£o de path cont√©m caracteres chineses (`Ë∑ØÂæÑ` = "caminho"), provavelmente copiados de output de modelo sem revis√£o. N√£o quebra funcionalidade mas indica falta de code review.

### üü° IMPORTANTE: Documenta√ß√£o 3.5x Maior Que Deveria

12.017 linhas de markdown contra 16.930 linhas de c√≥digo. Raz√£o docs:c√≥digo = 0.71. M√∫ltiplos planos desatualizados coexistem: `plano-implantacao-vps-agente-v2.md` (2.620 linhas), `agentvps-v2-roadmap.md` (35K), `agentvps-fase0-estabilizacao.md` (32K), 12+ arquivos em `plans/`. Grande parte est√° obsoleta mas nunca removida.

---

## 3. O Que Funciona Bem

### ‚úÖ Skill Registry ‚Äî Bem Implementado

O `registry.py` (203 linhas) √© clean code. Auto-discovery via filesystem, carregamento din√¢mico com `importlib.util`, busca por trigger exato ‚Üí parcial ‚Üí nome, singleton com lazy init, reload para desenvolvimento. O `base.py` define SkillBase abstrata com SecurityLevel enum ‚Äî extens√≠vel e clara.

Adicionar um skill novo realmente requer apenas criar diret√≥rio + `handler.py` + `config.yaml`. O requisito "1 arquivo, ~30 linhas" do plano da sprint foi atingido.

### ‚úÖ node_execute Refatorado

De ~250 linhas de if/elif para ~80 linhas que delegam ao registry. O fluxo plan ‚Üí skill lookup ‚Üí execute ‚Üí fallback est√° correto e extens√≠vel. Boa estrat√©gia de fallback: plano ‚Üí tool_suggestion ‚Üí trigger na mensagem ‚Üí smart_response.

### ‚úÖ Classifica√ß√£o de Seguran√ßa do Shell Exec

Os regex patterns para FORBIDDEN/DANGEROUS/SAFE s√£o bem pensados. Fork bomb, pipe-to-shell, dd, mkfs est√£o bloqueados. Docker management requer approval. Leitura √© SAFE. Default √© MODERATE. O `classify_command()` √© o melhor c√≥digo do projeto.

### ‚úÖ File Manager com Path Validation

`is_path_allowed()` resolve symlinks via `os.path.realpath()` (previne path traversal), verifica forbidden paths, e tem listas separadas para leitura e escrita. Correto e seguro.

### ‚úÖ Estrutura de Skills Padronizada

Todos os 10 skills seguem o mesmo pattern: `config.yaml` com metadata + `handler.py` com classe que herda `SkillBase`. Os configs s√£o consistentes (name, description, version, security_level, triggers, parameters, max_output_chars, timeout_seconds, enabled).

---

## 4. OpenClaw vs AgentVPS ‚Äî Compara√ß√£o Atualizada

### OpenClaw em Fevereiro 2026: Status

O OpenClaw est√° agora em 180.000+ stars mas sua situa√ß√£o de seguran√ßa continua grave:

- **CVE-2026-25253** (CVSS 8.8): 1-click RCE via token exfiltration ‚Äî corrigido em v2026.1.29 mas exp√¥s que safety features s√£o bypass√°veis via API
- **CVE-2026-21636**: Permission model bypass ‚Äî segundo CVE em semanas
- **386 skills maliciosos** no ClawHub (de ~3.286 totais = 11.7% maliciosos)
- **42.000+ inst√¢ncias expostas** (subiu de 30k para 42k)
- **v2026.2.12**: 40+ patches de seguran√ßa em uma release ‚Äî indicativo de d√≠vida t√©cnica massiva
- **Simon Willison** (criador do termo "prompt injection") identificou a "tr√≠ade letal": acesso a dados privados + exposi√ß√£o a conte√∫do n√£o confi√°vel + capacidade de comunica√ß√£o externa
- **Palo Alto Networks** classificou como "a maior insider threat potencial de 2026"

### Compara√ß√£o Funcional Atualizada

| Capacidade | OpenClaw | AgentVPS | Delta |
|---|---|---|---|
| Skills dispon√≠veis | 3.286 (11.7% maliciosos) | 10 (100% auditados) | OpenClaw 328x mais, mas com supply chain attack |
| Canais de chat | WhatsApp, Telegram, Discord, Signal, Slack, iMessage | Telegram | OpenClaw 6x mais |
| Execu√ß√£o de shell | ‚úÖ Irrestrita (configur√°vel) | ‚úÖ Com classifica√ß√£o de seguran√ßa 4 n√≠veis | AgentVPS mais seguro |
| Busca web | ‚úÖ Browser control | ‚úÖ Brave Search API | OpenClaw mais capaz |
| Self-improvement | ‚úÖ Funcional ‚Äî escreve e instala seus pr√≥prios skills | ‚ùå Placeholder | OpenClaw funcional |
| Mem√≥ria persistente | ‚úÖ Across conversations | ‚ö†Ô∏è PostgreSQL mas sem semantic search | Empate parcial |
| Comunidade | 180k stars, 500+ contributors | Solo project | Incompar√°vel |
| Seguran√ßa auditada | ‚ùå 3 CVEs em 3 semanas, sem security team dedicado | ‚úÖ Zero CVEs, 3-level allowlist | AgentVPS mais seguro |
| RAM | 2-4GB m√≠nimo (Node.js) | Desenhado para 2.4GB | AgentVPS mais eficiente |
| Stack | Node.js/TypeScript | Python/LangGraph | Prefer√™ncia pessoal |

### A Quest√£o Central: Migrar ou Continuar?

**An√°lise fria dos fatos:**

O OpenClaw faz coisas que o AgentVPS levaria meses para igualar. Self-improvement real, integra√ß√£o com 50+ plataformas, browser control, persist√™ncia cross-conversation com semantic search. Sua comunidade resolve bugs em horas e cria integra√ß√µes diariamente.

Mas o OpenClaw foi descrito por pesquisadores de seguran√ßa como "o maior insider threat potencial de 2026". A arquitetura fundamental permite que o agente desative suas pr√≥prias prote√ß√µes via API. 11.7% das skills na sua marketplace distribuem malware. E o Palo Alto Networks ‚Äî uma das maiores empresas de cybersecurity do mundo ‚Äî emitiu alerta formal.

**Recomenda√ß√£o: AINDA continuar com AgentVPS, mas mudar a estrat√©gia de desenvolvimento radicalmente.**

Raz√µes:
1. Seu caso de uso √© gerenciar uma VPS de produ√ß√£o com dados de clientes (fleet leasing). O risco de um CVE no OpenClaw expondo dados corporativos √© inaceit√°vel.
2. O OpenClaw roda Node.js e requer 2-4GB m√≠nimo. Sua VPS tem 2.4GB. Ficaria no limite antes de adicionar PostgreSQL + Redis.
3. O problema do AgentVPS n√£o √© falta de features ‚Äî √© falta de intelig√™ncia. Adicionar mais skills hardcoded n√£o resolve. O que resolve √© fazer o agente pensar, e isso √© o tema da se√ß√£o seguinte.

**Exce√ß√£o: vale monitorar e estudar os patterns do OpenClaw**, especialmente:
- Como implementam self-improvement real (skill writing loop)
- Como fazem context window management (mem√≥ria cross-session)
- A arquitetura de SOUL.md (system prompt persistente)

---

## 5. O Problema Fundamental: "Bot√µes Pr√©-Codificados"

Voc√™ identificou o problema central com precis√£o cir√∫rgica. Vou diagnosticar em profundidade.

### Sintoma

Quando algu√©m diz "tem o Node.js instalado?", o agente:
1. Classifica intent via LLM (gasta tokens) ‚Üí "task"
2. Planeja via node_plan ‚Üí skill="shell_exec"
3. Dentro do shell_exec, roda `_interpret_and_generate_command` que:
   - Testa 20+ padr√µes hardcoded com string matching
   - Se match: gera comando (ex: "which nodejs")
   - Se n√£o match: chama o LLM NOVAMENTE para interpretar (gasta mais tokens)
4. Executa o comando
5. Dentro do shell_exec, roda 15 blocos if/elif hardcoded para formatar resposta conversacional

S√£o 2 chamadas LLM + 35 blocos if/elif para responder uma pergunta simples.

Quando algu√©m diz "o Node est√° na m√°quina?" ‚Äî nenhum dos 20 patterns hardcoded reconhece "na m√°quina". O agente cai no fallback LLM. Funciona, mas com lat√™ncia dobrada e custo dobrado.

### Causa Raiz

O agente n√£o pensa. Ele faz matching de strings e delega para fun√ß√µes hardcoded. A "intelig√™ncia" √© simulada via if/elif, n√£o via racioc√≠nio. Cada novo caso de uso requer editar c√≥digo.

Isso √© fundamentalmente diferente de um sistema inteligente onde:
1. O LLM entende a inten√ß√£o em QUALQUER formula√ß√£o
2. O LLM decide qual ferramenta usar e com quais par√¢metros
3. O LLM interpreta o resultado e responde naturalmente

### O Que o AgentVPS Deveria Fazer (Modo Inteligente)

```
Usu√°rio: "tem o Node.js na m√°quina?"

LLM (1 chamada s√≥, com function calling):
  thought: "O usu√°rio quer saber se Node.js est√° instalado"
  tool_call: shell_exec(command="which node || node --version")

[Executa comando, retorna output]

LLM (1 chamada para responder):
  "Sim, Node.js v22.3.0 est√° instalado em /usr/local/bin/node"
```

2 chamadas LLM, 0 heur√≠sticas hardcoded, funciona para QUALQUER formula√ß√£o.

### Como Implementar (ReAct Pattern com Function Calling)

O LangGraph j√° suporta isso nativamente. O grafo deveria:

1. Receber mensagem
2. Enviar ao LLM COM a lista de tools dispon√≠veis (via function calling / tool use)
3. O LLM decide: responder diretamente OU chamar uma tool
4. Se chamou tool: executar ‚Üí retornar resultado ao LLM ‚Üí LLM gera resposta final
5. Se n√£o chamou tool: LLM responde diretamente

Isso elimina `node_plan`, `node_classify_intent` como blocos separados, e as heur√≠sticas do shell_exec. O LLM faz toda a interpreta√ß√£o.

O Gemini 2.5 Flash (j√° configurado no OpenRouter) suporta function calling nativamente. O custo √© similar porque j√° estamos fazendo 2 chamadas LLM por mensagem com o sistema atual.

---

## 6. Autonomous Blueprint: Diagn√≥stico Real

### O Que o Blueprint Define (6 Passos)

```
1. DETECT  ‚Üí Trigger identifica condi√ß√£o
2. PROPOSE ‚Üí Cria proposal com a√ß√£o sugerida
3. FILTER  ‚Üí Cap Gates verificam recursos/seguran√ßa/custo
4. EXECUTE ‚Üí Worker dedicado executa via Skill
5. COMPLETE ‚Üí Emite evento com resultado
6. RE-TRIGGER ‚Üí Evento gera novas proposals
```

### O Que Foi Implementado

```
1. DETECT  ‚Üí ‚ö†Ô∏è 6 triggers existem mas 4 de 6 t√™m condition=lambda: True (sempre disparam)
2. PROPOSE ‚Üí ‚ö†Ô∏è ram_high salva proposal no Redis mas ningu√©m a processa
3. FILTER  ‚Üí ‚ùå N√£o existe (sem cap gates)
4. EXECUTE ‚Üí ‚ö†Ô∏è Trigger executa a√ß√£o diretamente (sem worker, sem fila)
5. COMPLETE ‚Üí ‚ùå N√£o emite eventos
6. RE-TRIGGER ‚Üí ‚ùå N√£o existe
```

### Gap Real

O engine.py implementa um sistema de cron jobs com nome diferente. A arquitetura de proposals/missions/policies que diferenciaria um agente aut√¥nomo de um scheduler ainda n√£o existe. As tabelas PostgreSQL planejadas (agent_proposals, agent_missions, agent_policies) n√£o foram criadas.

---

## 7. Scores Atualizados

| Dimens√£o | v2 | v3 | Tend√™ncia | Justificativa |
|---|---|---|---|---|
| Arquitetura | 8/10 | 7/10 | ‚¨áÔ∏è | Skill Registry bom, mas shell_exec virou mega-m√≥dulo que reimplementa o grafo. Dual registry persiste. |
| Seguran√ßa | 7/10 | 5/10 | ‚¨áÔ∏è | API key exposta no GitHub p√∫blico. Debug log em /tmp. Classifica√ß√£o de seguran√ßa do shell_exec √© boa. |
| Funcionalidade | 3/10 | 5/10 | ‚¨ÜÔ∏è | 10 skills vs 5 anteriores. Shell exec e file manager s√£o √∫teis. Web search funcional. |
| Testes | 6/10 | 6/10 | ‚û°Ô∏è | +175 linhas (test_skill_registry) mas cobertura dos novos skills √© zero. |
| Qualidade de C√≥digo | 5/10 | 4/10 | ‚¨áÔ∏è | shell_exec com 397 linhas fazendo 3 trabalhos. Caractere Unicode corrompido. system_tools n√£o limpo. |
| Documenta√ß√£o | 7/10 | 5/10 | ‚¨áÔ∏è | 12k linhas de docs com planos obsoletos. Raz√£o docs:c√≥digo alta demais sem curadoria. |
| DevOps | 8/10 | 8/10 | ‚û°Ô∏è | Inalterado. CI/CD, Docker, pyproject.toml, ruff. |
| Autonomia | 2/10 | 3/10 | ‚¨ÜÔ∏è | Engine existe com 6 triggers, mas sem proposals/missions/cap gates = cron, n√£o autonomia. |
| Intelig√™ncia | N/A | 2/10 | üÜï | Novo crit√©rio. 35 blocos if/elif no shell_exec. Sem function calling. LLM usado como tradutor, n√£o como raciocinador. |
| **OVERALL** | **5.5/10** | **5.0/10** | ‚¨áÔ∏è | Funcionalidade subiu mas qualidade desceu. API key exposta √© grave. Shell_exec virou anti-pattern. |

### Diagn√≥stico v3

**"Motor instalado, mas com carburador em vez de inje√ß√£o eletr√¥nica."**

A sprint entregou funcionalidade real ‚Äî o agente agora faz 10 coisas em vez de 5. Mas o padr√£o de desenvolvimento replicou o problema antigo em escala maior. Cada skill resolve um caso espec√≠fico com heur√≠sticas hardcoded, em vez de usar o LLM como motor de racioc√≠nio.

O shell_exec √© o sintoma mais claro: 397 linhas de if/elif para mapear linguagem natural ‚Üí comando. Isso deveria ser 1 chamada de function calling ao LLM. Os 15 formatadores de resposta (RAM, containers, disco, hostname, uptime, etc.) deveriam ser 1 prompt ao LLM: "dado este output de terminal, responda a pergunta do usu√°rio."

A nota global caiu de 5.5 para 5.0 n√£o porque houve regress√£o funcional (houve progresso), mas porque novos problemas surgiram (API key exposta, mega-m√≥dulo, engine sem blueprint) e problemas antigos n√£o foram resolvidos (cleanup n√£o feito, dual entry point, docs obsoletas).

---

## 8. Top 5 A√ß√µes de Maior Impacto

### A√ß√£o 1: Implementar ReAct com Function Calling (TRANSFORMACIONAL)

**O que:** Substituir o fluxo `classify ‚Üí plan ‚Üí execute(heur√≠sticas)` por `LLM tool_use ‚Üí execute ‚Üí LLM response`.

**Por que:** Elimina 100% das heur√≠sticas hardcoded. O agente entende QUALQUER formula√ß√£o. Novas capacidades s√£o adicionadas apenas registrando fun√ß√µes como tools ‚Äî sem editar c√≥digo de roteamento.

**Como:** O Gemini 2.5 Flash via OpenRouter suporta function calling. Cada skill do registry vira uma tool function com name, description e parameters definidos. O LLM decide quando e como chamar cada tool.

**Impacto:** Transforma o agente de "bot√µes pr√©-codificados" para "racioc√≠nio real". A nota de Intelig√™ncia iria de 2/10 para 7/10.

**Esfor√ßo:** ~20h

### A√ß√£o 2: Corrigir Exposi√ß√£o de API Key AGORA

**O que:** Remover default value do BRAVE_API_KEY. Revogar key atual. Gerar nova. Auditar todo o c√≥digo por outros segredos.

**Esfor√ßo:** 1h. Sem desculpa para n√£o fazer imediatamente.

### A√ß√£o 3: Decompor shell_exec em responsabilidades separadas

**O que:** Mover interpreta√ß√£o para o grafo (ou function calling). Mover formata√ß√£o para o LLM. Handler fica com ~60 linhas: classificar seguran√ßa + executar subprocess + retornar output raw.

**Impacto:** -337 linhas em shell_exec. Elimina duplica√ß√£o de trabalho com o grafo.

**Esfor√ßo:** 8h (se combinado com A√ß√£o 1, fica impl√≠cito)

### A√ß√£o 4: Implementar Autonomous Blueprint Real (com PostgreSQL)

**O que:** Criar as tabelas de proposals/missions/policies. Refatorar engine.py para criar proposals ‚Üí verificar cap gates ‚Üí executar ‚Üí emitir eventos.

**Impacto:** Autonomia sobe de 3/10 para 6/10. Agente prop√µe a√ß√µes e pede confirma√ß√£o.

**Esfor√ßo:** 16h

### A√ß√£o 5: Cleanup t√©cnico pendente

**O que:** Deletar intent_classifier.py, semantic_memory.py (arquivos fantasma com coment√°rio DEPRECATED), remover system_tools.py TOOLS_REGISTRY, remover debug log de /tmp, limpar docs obsoletas.

**Esfor√ßo:** 4h

---

## 9. Prompt Para Valida√ß√£o Cross-Model

```
Voc√™ √© um avaliador t√©cnico s√™nior de software. Preciso de uma avalia√ß√£o extremamente detalhada, criteriosa e realista de um projeto no GitHub.

INSTRU√á√ïES:
1. Clone e leia TODOS os arquivos Python do reposit√≥rio. N√ÉO confie no README.
2. Para CADA arquivo, verifique se o c√≥digo realmente funciona ou √© placeholder/stub.
3. Conte linhas reais de c√≥digo funcional vs coment√°rios/docstrings/imports.
4. Identifique padr√µes (ou anti-padr√µes) recorrentes.

REPOSIT√ìRIO: https://github.com/Guitaitson/AgentVPS

AVALIE ESTAS DIMENS√ïES (0-10 cada):

1. **QUALIDADE DE C√ìDIGO** ‚Äî Duplica√ß√µes? Mega-m√≥dulos? Dead code? Consist√™ncia de estilo?
   - Verificar especificamente: core/skills/_builtin/shell_exec/handler.py (397 linhas ‚Äî √© um mega-m√≥dulo?)
   - Verificar: core/tools/system_tools.py (ainda existe? √© usado? deveria existir?)
   - Verificar: core/vps_langgraph/intent_classifier.py (deletado ou ainda existe?)

2. **ARQUITETURA** ‚Äî O grafo LangGraph faz sentido? Os n√≥s t√™m responsabilidades claras ou h√° sobreposi√ß√£o?
   - Verificar: node_classify_intent classifica com LLM, mas shell_exec reclassifica internamente ‚Äî isso √© correto?
   - Verificar: node_plan roteia por intent, mas shell_exec tem 20+ heur√≠sticas internas de roteamento ‚Äî duplica√ß√£o?

3. **FUNCIONALIDADE REAL** ‚Äî O que realmente funciona quando voc√™ envia uma mensagem?
   - Testar mentalmente: "execute ls -la" ‚Üí passa pelo grafo ‚Üí chega ao shell_exec ‚Üí funciona?
   - Testar mentalmente: "tem o docker instalado?" ‚Üí passa pelo grafo ‚Üí shell_exec reconhece? ou cai no fallback LLM?
   - Quantos dos 10 skills realmente executam a√ß√µes vs retornam strings hardcoded?

4. **SEGURAN√áA** ‚Äî Alguma API key exposta? Secrets em c√≥digo? Logs com dados sens√≠veis? Paths sem valida√ß√£o?
   - Verificar: web_search/handler.py tem API key hardcoded?
   - Verificar: nodes.py escreve debug log em /tmp?

5. **INTELIG√äNCIA vs BOT√ïES** ‚Äî O agente PENSA ou apenas faz string matching?
   - Contar quantos blocos if/elif existem no shell_exec para mapear linguagem natural
   - O agente usa function calling / tool use do LLM ou traduz manualmente?
   - Se eu perguntar a mesma coisa de 5 formas diferentes, quantas funcionam vs falham?

6. **AUTONOMOUS LOOP** ‚Äî O core/autonomous/engine.py implementa o blueprint de 6 passos?
   - Existe tabela de proposals? Existe cap gates? Existe emiss√£o de eventos?
   - Os triggers usam condi√ß√µes reais ou condition=lambda: True?

7. **COMPARA√á√ÉO COM OPENCLAW** ‚Äî Dado que OpenClaw tem 180k stars, 3000+ skills, self-improvement funcional, MAS 3 CVEs em 3 semanas e foi chamado de "maior insider threat de 2026" pelo Palo Alto Networks:
   - Faz sentido migrar para OpenClaw para uso em VPS de produ√ß√£o com dados corporativos?
   - Que padr√µes do OpenClaw valem a pena adaptar SEM herdar os problemas?

8. **TOP 5 A√á√ïES** ‚Äî Liste as 5 coisas que teriam maior impacto no projeto, com estimativa de esfor√ßo.

FORMATO DE SA√çDA:
- Para cada dimens√£o: nota 0-10, 3-5 par√°grafos de an√°lise com file paths espec√≠ficos
- Nota overall com m√©dia ponderada (Intelig√™ncia e Funcionalidade pesam 2x)
- Diagn√≥stico em uma frase
- Compara√ß√£o expl√≠cita: "Antes desta sprint era X, agora √© Y, mas deveria ser Z"
```
