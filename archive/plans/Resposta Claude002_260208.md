Li o documento. Ele complementa a Fase 0 com um Ã¢ngulo diferente: o anterior era sobre bugs (coisas quebradas), este Ã© sobre capacidade (coisas que faltam). Vou responder tudo com clareza sobre o que fazer agora vs. o que Ã© v2.
Mas primeiro, uma observaÃ§Ã£o importante: este documento descreve o problema central de forma diferente do anterior. Deixa eu ser direto sobre isso.

O problema central descrito aqui â€” "o agente classifica intents mas nÃ£o executa aÃ§Ãµes concretas" â€” nÃ£o Ã© um bug da Fase 0. Ã‰ a razÃ£o de existir da Fase 2 inteira (Skill Registry + Skills Core + Action Classification). A Fase 0 resolve o que estÃ¡ quebrado. A Fase 2 resolve o que estÃ¡ faltando.
Dito isso, vou responder as 6 perguntas e entregar os 5 itens pedidos:
1. Arquitetura de MemÃ³ria
A v2 jÃ¡ tem isso planejado em F3-04 (Hierarchical Memory), mas vou ser concreto sobre como funciona:
TrÃªs camadas, trÃªs propÃ³sitos:
Camada EpisÃ³dica (JSONL) â€” "o que aconteceu recentemente"

Transcripts das Ãºltimas conversas (7 dias)
Formato: JSONL append-only, um arquivo por dia
Consulta: grep/busca textual simples
Custo RAM: ~0 (leitura sob demanda do disco)

Camada SemÃ¢ntica (Qdrant) â€” "o que aprendi"

Fatos importantes extraÃ­dos das conversas: "API X nÃ£o funcionou, usar Y", "usuÃ¡rio prefere respostas curtas"
Embeddings via API (Voyage AI ou OpenAI, sem modelo local = 0 RAM)
Consulta: similarity search por embedding da mensagem atual
Custo RAM: ~400MB quando Qdrant estÃ¡ rodando (sob demanda)

Camada Perfil (PostgreSQL) â€” "quem Ã© o usuÃ¡rio e o que Ã© o sistema"

PreferÃªncias, configs, credenciais (encrypted), capabilities registradas
Formato: tabelas relacionais, sempre disponÃ­vel
Custo RAM: ~200MB (jÃ¡ rodando)

Como distinguir recente vs. longo prazo:

Todo transcript vai para JSONL (episÃ³dico)
Um job periÃ³dico (F4-08 Memory Compaction) analisa transcripts >7 dias, extrai fatos importantes, salva no Qdrant (semÃ¢ntico), e arquiva o JSONL
PreferÃªncias detectadas ("usuÃ¡rio sempre pede em portuguÃªs") vÃ£o direto para PostgreSQL (perfil)

Como aprender com falhas:
PostgreSQL: tabela "learnings"
| id | category | trigger | lesson | created_at |
|----|----------|---------|--------|------------|
| 1  | api_failure | github_api_rate_limit | "Usar token PAT, nÃ£o basic auth" | 2026-02-08 |
| 2  | tool_choice | web_search | "Brave Search > Google Custom Search para queries tÃ©cnicas" | 2026-02-09 |
Quando o agente vai executar uma aÃ§Ã£o, consulta learnings relevantes antes. Isso Ã© simples (SQL query) e nÃ£o precisa de Qdrant.
Quando implementar: NÃ£o agora. PostgreSQL para learnings pode ser adicionado na Fase 0 como extensÃ£o do F0-02 (Ã© uma tabela simples). Qdrant e compaction ficam para F3-04.

2. ExecuÃ§Ã£o AutÃ´noma de Ferramentas
Esse Ã© o cenÃ¡rio-chave do documento. Vou ser bem especÃ­fico:
O fluxo correto para "Liste meus projetos no GitHub":
FASE ATUAL (v1 â€” quebrado):
  classify("Liste meus projetos no GitHub")
  â†’ intent: "task" ou "self_improve"
  â†’ respond: "NÃ£o tenho ferramenta direta para GitHub"
  â†’ FIM (inÃºtil)

APÃ“S FASE 2 (v2 â€” funcional):
  classify("Liste meus projetos no GitHub")
  â†’ intent: "task"
  â†’ skill_lookup("github", "list projects")
  â†’ skill encontrado? 
     SIM â†’ executar skill â†’ retornar resultado
     NÃƒO â†’ registrar necessidade â†’ informar usuÃ¡rio:
            "NÃ£o tenho skill de GitHub ainda. Posso instalar? [Sim/NÃ£o]"
     â†’ se Sim:
        â†’ classificar aÃ§Ã£o: "instalar MCP" = nÃ­vel DANGEROUS
        â†’ pedir aprovaÃ§Ã£o via Telegram (botÃ£o)
        â†’ se aprovado: instalar, configurar, executar
        â†’ se rejeitado: sugerir alternativa manual
A pergunta real Ã©: isso deveria ser Fase 0 ou Fase 2?
Resposta: Fase 2. E por uma razÃ£o tÃ©cnica concreta â€” para fazer isso funcionar, vocÃª precisa de:

Skill Registry (F2-01) â€” para o agente saber quais ferramentas tem
Action Classification (F2-03) â€” para classificar "instalar MCP" como dangerous
Approval Workflow (F2-03) â€” para pedir confirmaÃ§Ã£o via Telegram

Sem esses trÃªs componentes, qualquer implementaÃ§Ã£o de "execuÃ§Ã£o autÃ´noma" seria frÃ¡gil e insegura. A Fase 0 garante que o fluxo bÃ¡sico funciona (classify â†’ respond sem crash). A Fase 2 dÃ¡ ao agente mÃ£os para agir.
O que PODE ser feito na Fase 0 como melhoria mÃ­nima:
Fazer o node_generate_response ser mais inteligente quando detecta que o usuÃ¡rio quer algo que o agente nÃ£o pode fazer:
python# Em vez de "nÃ£o tenho ferramenta direta"
# Responder com plano de aÃ§Ã£o:
response = (
    "Ainda nÃ£o tenho a ferramenta de GitHub instalada. "
    "Para implementar isso, vou precisar:\n"
    "1. Instalar o MCP de GitHub\n"
    "2. Configurar seu token PAT\n"
    "3. Testar a conexÃ£o\n"
    "Quer que eu registre isso como prÃ³xima melhoria?"
)
```

Isso Ã© cosmÃ©tico (muda a resposta, nÃ£o a capacidade), mas jÃ¡ Ã© melhor UX.

---

### 3. Hierarquia de Ferramentas

A hierarquia proposta no documento estÃ¡ boa. Vou refinar com base no que jÃ¡ temos no roadmap:
```
NÃVEL 0 â€” SAFE (automÃ¡tico, sem confirmaÃ§Ã£o)
â”œâ”€â”€ Ler arquivos
â”œâ”€â”€ Consultar PostgreSQL/Redis
â”œâ”€â”€ Checar status de containers
â”œâ”€â”€ Listar processos
â””â”€â”€ Consultar memÃ³ria/learnings

NÃVEL 1 â€” MODERATE (executa, mas loga tudo)
â”œâ”€â”€ Criar/editar arquivos em diretÃ³rios permitidos
â”œâ”€â”€ Instalar pacotes pip/apt (de whitelist)
â”œâ”€â”€ Chamadas a APIs externas (GitHub, web search)
â”œâ”€â”€ Enviar mensagens por outros canais
â””â”€â”€ Criar containers Docker (com resource limits)

NÃVEL 2 â€” DANGEROUS (requer aprovaÃ§Ã£o via Telegram)
â”œâ”€â”€ Deletar arquivos/containers
â”œâ”€â”€ Executar scripts arbitrÃ¡rios
â”œâ”€â”€ Modificar configuraÃ§Ãµes do sistema
â”œâ”€â”€ Instalar novos MCPs
â”œâ”€â”€ Modificar cÃ³digo do prÃ³prio agente
â””â”€â”€ OperaÃ§Ãµes de git (commit, push, merge)

NÃVEL 3 â€” FORBIDDEN (nunca, nem com aprovaÃ§Ã£o)
â”œâ”€â”€ rm -rf /
â”œâ”€â”€ Desabilitar firewall
â”œâ”€â”€ Expor portas sem autenticaÃ§Ã£o
â”œâ”€â”€ Modificar credenciais de root
â””â”€â”€ Desabilitar o prÃ³prio agente
ImplementaÃ§Ã£o: Isso Ã© o job F2-03 (Action Classification & Approval). Cada skill declara seu nÃ­vel no config.yaml:
yaml# skills/github-api/config.yaml
name: github-api
level: moderate  # executa sem perguntar, mas loga
actions:
  list_repos: safe
  create_repo: moderate
  delete_repo: dangerous  # override por aÃ§Ã£o especÃ­fica
O approval flow via Telegram:
python# Quando aÃ§Ã£o Ã© DANGEROUS:
await telegram.send_message(
    admin_chat_id,
    f"ğŸ”´ AÃ§Ã£o DANGEROUS solicitada:\n"
    f"Skill: {skill_name}\n"
    f"AÃ§Ã£o: {action}\n"
    f"Contexto: {user_message}\n",
    reply_markup=InlineKeyboardMarkup([
        [InlineKeyboardButton("âœ… Aprovar", callback_data=f"approve_{action_id}")],
        [InlineKeyboardButton("âŒ Rejeitar", callback_data=f"reject_{action_id}")],
    ])
)
# Timeout de 5 minutos â€” se nÃ£o aprovar, rejeita automaticamente
```

---

### 4. IntegraÃ§Ã£o MCP

Preciso ser cÃ©tico aqui porque MCP Ã© uma Ã¡rea onde hÃ¡ muito hype e pouca maturidade.

**Estado atual:** VocÃª tem `mcp_server.py` (FastAPI-MCP), mas isso faz do AgentVPS um **MCP server** (expÃµe ferramentas para outros). O que vocÃª quer Ã© o contrÃ¡rio: o AgentVPS como **MCP client** (consome ferramentas de outros servidores MCP).

**Realidade sobre MCPs em fevereiro 2026:**
- MCPs de GitHub, Brave Search, filesystem existem e funcionam
- A maioria roda como processos separados (cada um consome RAM)
- Com 2.4GB, **nÃ£o dÃ¡ para ter muitos MCPs rodando simultaneamente**

**Abordagem recomendada (pragmÃ¡tica):**

Em vez de MCPs para tudo, usar uma hierarquia de complexidade:
```
PRIORIDADE 1 â€” Implementar como skills nativos (sem MCP overhead):
â”œâ”€â”€ shell-exec (subprocess.run â€” 0 RAM extra)
â”œâ”€â”€ file-manager (os/pathlib â€” 0 RAM extra)
â”œâ”€â”€ web-search (httpx â†’ Brave Search API â€” 0 RAM extra)
â”œâ”€â”€ github-api (httpx â†’ GitHub REST API â€” 0 RAM extra)
â””â”€â”€ memory-query (asyncpg â†’ PostgreSQL â€” 0 RAM extra)

PRIORIDADE 2 â€” Usar MCPs apenas quando a alternativa nativa Ã© ruim:
â”œâ”€â”€ Kilocode/Claude Code (MCP faz sentido â€” CLI complexo)
â””â”€â”€ n8n (jÃ¡ roda via Docker, API REST Ã© suficiente)

PRIORIDADE 3 â€” Avaliar caso a caso:
â””â”€â”€ Qualquer ferramenta nova: primeiro tentar skill nativo,
    sÃ³ usar MCP se o skill nativo for muito complexo
Por que skills nativos > MCPs?

0 RAM extra (roda no processo do agente)
Sem overhead de comunicaÃ§Ã£o JSON-RPC
Mais fÃ¡cil de debugar
Mais fÃ¡cil de testar

MCPs fazem sentido quando o serviÃ§o Ã© complexo o suficiente para justificar um processo separado.
Descoberta de MCPs disponÃ­veis:
python# No Skill Registry (F2-01), cada skill pode ser:
# - native: funÃ§Ã£o Python no processo
# - mcp: servidor MCP externo
# - docker: container sob demanda

# skills/registry.py
class SkillType(Enum):
    NATIVE = "native"      # FunÃ§Ã£o Python, 0 RAM extra
    MCP = "mcp"            # Servidor MCP externo
    DOCKER = "docker"      # Container sob demanda
```

---

### 5. Resource Management (RAM)

Budget atualizado com cenÃ¡rios:
```
CENÃRIO A â€” Modo Normal (sem Qdrant, sem n8n)
  OS + sistema:     ~200 MB
  PostgreSQL:       ~200 MB
  Redis:            ~50 MB
  Python (agente):  ~300 MB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:            ~750 MB
  Livre:            ~1650 MB
  
CENÃRIO B â€” Com Qdrant (busca semÃ¢ntica ativa)
  CenÃ¡rio A:        ~750 MB
  Qdrant:           ~400 MB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:            ~1150 MB
  Livre:            ~1250 MB

CENÃRIO C â€” Qdrant + n8n (automaÃ§Ã£o ativa)
  CenÃ¡rio B:        ~1150 MB
  n8n:              ~300 MB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:            ~1450 MB
  Livre:            ~950 MB

CENÃRIO D â€” MÃXIMO (tudo ligado)
  CenÃ¡rio C:        ~1450 MB
  Kilocode/outro:   ~500 MB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:            ~1950 MB
  Livre:            ~450 MB âš ï¸ ZONA DE RISCO
Regras de gestÃ£o:

MÃ­nimo livre: 500 MB. Se livre < 500MB, recusar novas ferramentas e desligar a menos crÃ­tica
Qdrant: sob demanda. Subir quando precisa de busca semÃ¢ntica, manter por 5min, descer se inativo
n8n: sob demanda. SÃ³ subir quando uma automaÃ§Ã£o precisa rodar
Nunca cenÃ¡rio D sem intervenÃ§Ã£o manual

Quando recusar request por falta de recursos:
pythonasync def check_resources_before_skill(skill: Skill) -> bool:
    free_ram = get_free_ram_mb()
    skill_needs = skill.config.ram_estimate_mb
    
    if free_ram - skill_needs < 500:  # margem mÃ­nima
        await telegram.send(
            f"âš ï¸ RAM insuficiente para {skill.name}.\n"
            f"Livre: {free_ram}MB, Precisa: {skill_needs}MB\n"
            f"Desligar algo primeiro? [Qdrant/n8n/Cancelar]"
        )
        return False
    return True
```

---

### 6. Self-Improvement AutÃ´nomo

**Na Fase 0:** O self_improve apenas **identifica e informa**. NÃ£o executa nada.

**Na v2 (F4-04):** O pipeline completo:
```
ETAPA 1 â€” DETECTAR (automÃ¡tico)
  â”œâ”€â”€ Analisar logs de "nÃ£o tenho ferramenta para X"
  â”œâ”€â”€ Contar frequÃªncia de pedidos nÃ£o atendidos
  â””â”€â”€ Rankear por impacto (quantas vezes pedido Ã— complexidade)

ETAPA 2 â€” PLANEJAR (automÃ¡tico)
  â”œâ”€â”€ Pesquisar como implementar (web search + learnings)
  â”œâ”€â”€ Gerar plano de implementaÃ§Ã£o
  â”œâ”€â”€ Estimar tempo e recursos
  â””â”€â”€ Enviar plano para aprovaÃ§Ã£o: "Detectei que vocÃª pede GitHub 5x/semana. 
       Posso implementar skill GitHub em ~2h. Aprovar?"

ETAPA 3 â€” IMPLEMENTAR (apÃ³s aprovaÃ§Ã£o humana)
  â”œâ”€â”€ Criar branch: self-improve/add-github-skill
  â”œâ”€â”€ Gerar cÃ³digo do skill (em sandbox Docker)
  â”œâ”€â”€ Rodar testes automatizados
  â””â”€â”€ Se testes passam: commit + enviar para review

ETAPA 4 â€” VALIDAR (apÃ³s aprovaÃ§Ã£o humana)
  â”œâ”€â”€ Merge para main
  â”œâ”€â”€ Recarregar Skill Registry
  â”œâ”€â”€ Testar end-to-end com mensagem real
  â””â”€â”€ Registrar learning: "GitHub skill implementado, funcional"

ETAPA 5 â€” DOCUMENTAR (automÃ¡tico)
  â”œâ”€â”€ Atualizar TOOLS.md
  â”œâ”€â”€ Adicionar ao CLI: agent-cli skills list
  â””â”€â”€ Logar no audit trail
```

**DiferenÃ§a crucial com a v1:** Na v1, `self_improve` tenta fazer tudo de uma vez e falha silenciosamente. Na v2, cada etapa tem checkpoint e validaÃ§Ã£o. Se falhar em qualquer ponto, loga, notifica, e para.

---

## Os 5 EntregÃ¡veis Pedidos

### EntregÃ¡vel 1 â€” Diagrama Atual vs. Proposta
```
â•â•â• ATUAL (v1 â€” quebrado) â•â•â•

Telegram â†’ Bot â†’ classify â†’ [intent?]
                               â”œâ”€â”€ chat â†’ LLM â†’ resposta genÃ©rica
                               â”œâ”€â”€ question â†’ LLM â†’ resposta genÃ©rica  
                               â”œâ”€â”€ command â†’ execute (shell) â†’ resposta
                               â”œâ”€â”€ task â†’ execute (limitado) â†’ resposta
                               â””â”€â”€ self_improve â†’ ??? â†’ NULL â† BUG

Problemas: sem tools, sem approval, sem fallback, self_improve quebrado


â•â•â• PROPOSTA (v1.5 = Fase 0 estabilizada) â•â•â•

Telegram â†’ Bot â†’ classify â†’ load_context â†’ plan â†’ [intent?]
                                                     â”œâ”€â”€ chat â†’ LLM â†’ respond â†’ save_memory
                                                     â”œâ”€â”€ question â†’ LLM â†’ respond â†’ save_memory
                                                     â”œâ”€â”€ command â†’ execute â†’ respond â†’ save_memory
                                                     â”œâ”€â”€ task â†’ execute â†’ respond â†’ save_memory
                                                     â””â”€â”€ self_improve â†’ check_capabilities 
                                                                         â†’ respond (informa plano)
                                                                         â†’ save_memory

DiferenÃ§a: fluxo funcional, sem NULL, sem crash. Mas ainda sem tools reais.


â•â•â• PROPOSTA (v2 â€” Fases 1-4) â•â•â•

Gateway (FastAPI)
  â”œâ”€â”€ Telegram Adapter
  â”œâ”€â”€ WhatsApp Adapter (Evolution API)
  â””â”€â”€ Webhook/API Adapter
       â†“
Session Router â†’ Context Window Guard â†’ LLM Provider (multi-model)
       â†“
Brain (LangGraph)
  â”œâ”€â”€ Prompt Composer (dinÃ¢mico)
  â”œâ”€â”€ Reasoning Validator (step-level)
  â””â”€â”€ Skill Registry â†’ [skill encontrado?]
       â”œâ”€â”€ SIM â†’ classificar nÃ­vel â†’ [safe/moderate/dangerous]
       â”‚          â”œâ”€â”€ safe â†’ executar â†’ respond
       â”‚          â”œâ”€â”€ moderate â†’ executar + log â†’ respond
       â”‚          â””â”€â”€ dangerous â†’ approval Telegram â†’ executar/rejeitar â†’ respond
       â””â”€â”€ NÃƒO â†’ [self-improve pipeline]
                   â†’ detectar necessidade
                   â†’ planejar implementaÃ§Ã£o
                   â†’ pedir aprovaÃ§Ã£o
                   â†’ implementar em sandbox
                   â†’ validar + deploy
```

### EntregÃ¡vel 2 â€” Fluxo "UsuÃ¡rio Pede Algo Novo"
```
Mensagem: "Liste meus projetos no GitHub"

1. Gateway recebe mensagem via Telegram
2. Session Router identifica sessÃ£o do usuÃ¡rio
3. Context Window Guard carrega contexto relevante (Ãºltimas msgs + learnings)
4. Brain classifica: intent = "task", tool_needed = "github"
5. Skill Registry busca: skills.search("github") â†’ NÃƒO ENCONTRADO
6. Brain gera resposta intermediÃ¡ria:
   "NÃ£o tenho skill de GitHub ainda. Posso instalar?"
   [BotÃ£o: Sim] [BotÃ£o: NÃ£o, faÃ§a manualmente]

SE USUÃRIO CLICA "SIM":
7. Action Classification: instalar_skill = DANGEROUS
8. Approval: jÃ¡ aprovado pelo "Sim" do passo 6
9. Self-Improve Pipeline:
   a. Pesquisa: "como acessar GitHub API via Python"
   b. Gera skill: skills/github-api/handler.py
   c. Testa em sandbox Docker
   d. Se teste OK: registra no Skill Registry
   e. Executa skill com a mensagem original
10. Resposta: "Seus projetos: [lista]"
11. Learning salvo: "GitHub skill funcional, usar httpx + REST API"

SE USUÃRIO CLICA "NÃƒO":
7. Brain sugere alternativa:
   "OK. VocÃª pode rodar manualmente: gh repo list --json name"
8. FIM
EntregÃ¡vel 3 â€” Hierarquia de Approvals
NÃ­velClassificaÃ§Ã£oAÃ§Ã£oExemploControle0SAFELeitura, consultaLer arquivo, query DB, checar statusAutomÃ¡tico1MODERATEEscrita limitadaCriar arquivo, chamar API, pip installAuto + log2DANGEROUSEscrita destrutiva / auto-modificaÃ§Ã£oDeletar, instalar MCP, modificar cÃ³digo, git pushAprovaÃ§Ã£o Telegram com timeout 5min3FORBIDDENDestruiÃ§Ã£o irreversÃ­velrm -rf, desabilitar firewall, expor portasBloqueado sempre
ImplementaÃ§Ã£o do approval:

Timeout: 5 minutos (configurÃ¡vel)
Default se timeout: REJEITAR
Log: toda decisÃ£o (aprovada/rejeitada/timeout) vai para audit trail
Override: comando /mode admin permite moderate sem perguntar (dangerous continua pedindo)

EntregÃ¡vel 4 â€” PriorizaÃ§Ã£o de Ferramentas
Em ordem de implementaÃ§Ã£o:
PrioridadeSkillTipoRAM ExtraFase1shell-execNative0F2-022file-managerNative0F2-023memory-queryNative0F2-024web-search (Brave API)Native0F2-025self-edit (cÃ³digo prÃ³prio)Native0F2-026github-apiNative0ApÃ³s F2-027qdrant-searchNative400MB (Qdrant)F3-048n8n-triggerDocker300MB (n8n)F3-10
Por que shell-exec primeiro? Porque com shell-exec + file-manager o agente pode fazer quase tudo â€” Ã© o "canivete suÃ­Ã§o". gh repo list Ã© um shell command. curl Ã© um shell command. A maioria das tarefas no VPS se resolve com shell.
Por que web-search antes de github-api? Porque web-search Ã© Ãºtil para mais cenÃ¡rios (pesquisa geral, troubleshooting, self-improvement research).
EntregÃ¡vel 5 â€” MÃ©tricas de Sucesso
Fase 0 (EstabilizaÃ§Ã£o):

âœ… 5/5 intents retornam resposta (nÃ£o None)
âœ… 0 crashes em 24h de operaÃ§Ã£o
âœ… CI/CD verde
âœ… Tempo mÃ©dio de resposta < 5s para chat/question

Fase 2 (Skills funcionais):

âœ… 5 skills core operacionais
âœ… Novo skill criÃ¡vel em < 30 minutos (por humano)
âœ… 0 aÃ§Ãµes DANGEROUS executadas sem aprovaÃ§Ã£o
âœ… WhatsApp funcional como segundo canal

Fase 3 (InteligÃªncia):

âœ… Custo mÃ©dio < $0.05/conversa
âœ… 0 downtime percebido de LLM (failover funcionando)
âœ… RAG query < 5s
âœ… Cache hit rate > 30% em perguntas repetitivas

Fase 4 (Autonomia):

âœ… 0 self-improvements sem aprovaÃ§Ã£o humana
âœ… Multi-agent delegation funcional
âœ… Skill auto-gerado passa em testes automatizados


A mensagem principal Ã©: seu documento descreve exatamente o que as Fases 2-4 resolvem. A Fase 0 Ã© "parar de crashar". As Fases 2-4 sÃ£o "comeÃ§ar a ser Ãºtil de verdade". A ordem importa â€” construir skills em cima de um grafo que retorna None Ã© construir em areia.